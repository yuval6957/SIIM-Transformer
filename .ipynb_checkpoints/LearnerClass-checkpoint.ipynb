{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pickle\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from exp.misc import *\n",
    "from exp.ProcessData import *\n",
    "from exp.PytorchModels import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "try:\n",
    "    import sandesh\n",
    "except:\n",
    "    print('warning sandesh module not imported')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast,GradScaler\n",
    "from tqdm import notebook\n",
    "\n",
    "class Learner(object):\n",
    "    def __init__(self,model,optimizer,loss_func,name=\"\",scheduler=None,device='cpu'):\n",
    "        self.model=model\n",
    "        self.optimizer=optimizer\n",
    "        self.loss_func=loss_func\n",
    "        self.scheduler=scheduler\n",
    "        self.scaler=None\n",
    "        self.device=device\n",
    "        self.metric=None\n",
    "        self.name=name\n",
    "        self.log={}\n",
    "        self.eth=0.99\n",
    "\n",
    "    def init_amp(self):\n",
    "        self.scaler=GradScaler()\n",
    "        \n",
    "    def get_y(self,batch):\n",
    "        # get Y from Batch, the default is batch[-1] but you can overwrite it\n",
    "        return batch[-1]\n",
    "\n",
    "    def get_inds(self,batch):\n",
    "        # get Y from Batch, the default is batch[-1] but you can overwrite it\n",
    "        return batch[-1]\n",
    " \n",
    "    def get_x(self,batch):\n",
    "        # get x from Batch, the default is batch[:-1] but you can overwrite it\n",
    "        if isinstance(batch,(list,tuple)):\n",
    "            return batch[:-1]\n",
    "        else:\n",
    "            return [batch]\n",
    "\n",
    "    def one_cycle(self,batch,train=True,do_step=True):\n",
    "        device = self.device\n",
    "        self.preprocess_batch(batch,train)\n",
    "        y_true=self.get_y(batch)\n",
    "        with autocast(self.scaler is not None):\n",
    "            y_pred= self.model(*(x.to(device) for x in self.get_x(batch)))\n",
    "            loss = self.loss_func(y_pred,y_true.to(device))\n",
    "        if train:\n",
    "            if self.scaler is not None:\n",
    "                self.scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            if do_step:\n",
    "                if self.scaler is not None:\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    self.optimizer.step()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "        return loss.item() if train else (loss.item(), y_pred.to('cpu').detach())\n",
    "\n",
    "\n",
    "    def one_training_epoch(self, dl, accumulation_steps=1):\n",
    "        device = self.device\n",
    "        torch.cuda.empty_cache()\n",
    "        avg_loss = 0.\n",
    "        lossf=0.\n",
    "        self.model=self.model.train()\n",
    "        self.model.zero_grad()\n",
    "        tk0 = notebook.tqdm(dl)\n",
    "        for i,batch in enumerate(tk0):\n",
    "            do_step = (i+1) % accumulation_steps == 0\n",
    "            loss_item = self.one_cycle(batch, train=True, do_step=do_step)\n",
    "            e=min(self.eth,1-1.0/(i+1.0))\n",
    "            lossf = e*lossf+(1-e)*loss_item \n",
    "            tk0.set_postfix(loss = lossf)\n",
    "            avg_loss += loss_item / len(dl)\n",
    "        tk0.disable=False\n",
    "        tk0.set_postfix(loss = avg_loss)\n",
    "        tk0.disable=True\n",
    "        return avg_loss\n",
    "\n",
    "    def agg_tta(self,y):\n",
    "        if y.shape[0]==1:\n",
    "            return y[0]\n",
    "        else:\n",
    "            return y.mean(0)\n",
    "        \n",
    "    def preprocess_batch(self,batch,train=True):\n",
    "        return(batch)\n",
    "        \n",
    "    def one_eval_epoch(self, dl,tta=1):\n",
    "        device = self.device\n",
    "        avg_loss = 0.\n",
    "        avg_accuracy = 0.\n",
    "        lossf=0\n",
    "        self.model=self.model.eval()\n",
    "        predss=[]\n",
    "        with torch.no_grad():\n",
    "            for t in range(tta):\n",
    "                pred_list=[]\n",
    "                true_list=[]\n",
    "                tk0 = notebook.tqdm(dl)\n",
    "                for i,batch in enumerate(tk0):\n",
    "                    loss_item, y_pred = self.one_cycle(batch, train=False, do_step=False)\n",
    "                    pred_list.append(y_pred.numpy())\n",
    "                    true_list.append(self.get_y(batch).numpy())\n",
    "                    e=min(self.eth,1-1.0/(i+1.0))\n",
    "                    lossf = e*lossf+(1-e)*loss_item \n",
    "                    tk0.set_postfix(loss = lossf)\n",
    "                    avg_loss += loss_item / len(dl)\n",
    "                y_true=np.concatenate(true_list,0)\n",
    "                predss.append(np.concatenate(pred_list,0))\n",
    "            preds=self.agg_tta(np.stack(predss,0)) if tta>1 else predss[0]\n",
    "            m= dict() if self.metric is None else self.metric(preds,y_true)\n",
    "        tk0.disable=False\n",
    "        tk0.set_postfix(loss = avg_loss, **m)\n",
    "        tk0.disable=True\n",
    "        return avg_loss, m\n",
    "    \n",
    "    def send_log(self,**kwargs):\n",
    "        log={'model':self.name}\n",
    "        log.update(kwargs)\n",
    "        if 'sandesh' in sys.modules:\n",
    "            sandesh.send(log)\n",
    "        else:\n",
    "            print('log not sent - no sandesh module')\n",
    "        \n",
    "    def save2log(self,**kwargs):\n",
    "        for key in kwargs.keys():\n",
    "            if key not in self.log:\n",
    "                self.log[key]=[]\n",
    "            self.log[key].append(kwargs[key])\n",
    "    \n",
    "    def evaluate(self,ds,num_workers=8,tta=1,dl_args={'shuffle':False}):\n",
    "            dl=D.DataLoader(ds,num_workers=num_workers,**dl_args)\n",
    "            return self.one_eval_epoch(dl,tta=tta)\n",
    "\n",
    "    \n",
    "    def fit(self,num_epoches,train_ds,validate_ds=None,batch_size=None,lr=None,accumulation_steps=1,\n",
    "            num_workers=8,send_log=True,eval_batch=None,reset_best=False,make_best=True,tta=1,\n",
    "            train_dl_args={'shuffle':True},val_dl_args={'shuffle':False},save_checkpoint='best',path=''):\n",
    "        if batch_size is not None:\n",
    "            train_dl_args['batch_size']=batch_size\n",
    "            val_dl_args['batch_size']=batch_size\n",
    "        if eval_batch is not None:\n",
    "            val_dl_args['batch_size']=eval_batch\n",
    "\n",
    "        tq = notebook.tqdm(range(num_epoches))\n",
    "        if lr is not None:\n",
    "            self.set_lr(lr)\n",
    "        if reset_best or not hasattr(self,'best_metric'):\n",
    "            self.best_model=None\n",
    "            self.best_metric=np.inf\n",
    "        for k,epoch in enumerate(tq):\n",
    "            self.on_epoch_begin(epoch,train_ds=train_ds,validate_ds=validate_ds)\n",
    "            dl=D.DataLoader(train_ds, num_workers=num_workers,**train_dl_args)\n",
    "            torch.cuda.empty_cache()\n",
    "            tavg_loss=self.one_training_epoch(dl,accumulation_steps=accumulation_steps)\n",
    "            if validate_ds is not None:\n",
    "                avg_loss , metric =self.evaluate(validate_ds,  \n",
    "                                 num_workers=num_workers,dl_args=val_dl_args, tta=tta)\n",
    "            else:\n",
    "                avg_loss=tavg_loss\n",
    "                metric={}\n",
    "            if send_log:\n",
    "                self.send_log(epoch=epoch,tloss=tavg_loss,loss=avg_loss,**metric)\n",
    "            self.save2log(epoch=epoch,tloss=tavg_loss,loss=avg_loss,**metric)\n",
    "            m = avg_loss  if 'metric' not in metric.keys() else metric['metric']\n",
    "            if save_checkpoint=='last':\n",
    "                self.save_checkpoint(path)\n",
    "            if m<self.best_metric:\n",
    "                self.best_metric=m\n",
    "                self.best_model = copy.deepcopy(self.model.state_dict())\n",
    "                tq.set_postfix(best_metric=self.best_metric)\n",
    "                if save_checkpoint=='best':\n",
    "                    self.save_checkpoint(path)\n",
    "            self.on_epoch_end(epoch)\n",
    "            \n",
    "        print ('best metric:',self.best_metric)\n",
    "        if make_best:\n",
    "            self.model.load_state_dict(self.best_model)\n",
    "        \n",
    "    def save_model(self,path,name=None):\n",
    "        name = self.name if name is None else name\n",
    "        torch.save(self.model.state_dict(),f'{path}{name}')\n",
    "        \n",
    "    def load_model(self,path,name=None):\n",
    "        name = self.name if name is None else name\n",
    "        self.model.load_state_dict(torch.load(f'{path}{name}'))\n",
    "        \n",
    "           \n",
    "    def save_checkpoint(self,path,name=None):\n",
    "        name = self.name+'.chk' if name is None else name\n",
    "        checkpoint={\n",
    "                'model': self.model.state_dict(),\n",
    "                'best_model': self.best_model,\n",
    "                'best_metric': self.best_metric,\n",
    "                'model': self.model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                'log' : self.log\n",
    "                }\n",
    "        if self.scaler:\n",
    "            checkpoint['scaler']=self.scaler.state_dict()\n",
    "        torch.save(checkpoint,f'{path}{name}')\n",
    "\n",
    "    def load_checkpoint(self,path,name=None):\n",
    "        name = self.name+'.chk' if name is None else name+'.chk'\n",
    "        checkpoint=torch.load(f'{path}{name}')\n",
    "        self.model.load_state_dict(checkpoint['model'])\n",
    "        self.best_model=checkpoint['best_model']\n",
    "        self.best_metric=checkpoint['best_metric']\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.log=checkpoint['log']\n",
    "        if 'scaler' in checkpoint.keys():\n",
    "            self.scaler=GradScaler()\n",
    "            self.scaler.load_state_dict(checkpoint['scaler'])\n",
    "        else:\n",
    "            self.scaler=None\n",
    "       \n",
    "    def set_lr(self,lr):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    def on_epoch_begin(self,*args,**kargs):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self,*args,**kargs):\n",
    "        pass\n",
    "    \n",
    "    def predict(self,ds,batch_size=None,num_workers=8,dl_args={'shuffle':False},return_inds=False,return_true=False,verbose=True,do_eval=True):\n",
    "        device = self.device\n",
    "        if batch_size is not None:\n",
    "            dl_args['batch_size']=batch_size     \n",
    "        dl=D.DataLoader(ds,num_workers=num_workers,**dl_args)\n",
    "        pred_list=[]\n",
    "        inds_list=[]\n",
    "        true_list=[]\n",
    "        if do_eval:\n",
    "            self.model=self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            tk0 = notebook.tqdm(dl) if verbose else dl\n",
    "            for i,batch in enumerate(tk0):\n",
    "                with autocast(self.scaler is not None):\n",
    "                    y_pred= self.model(*(x.to(device) for x in self.get_x(batch)))\n",
    "                if return_inds:\n",
    "                    inds_list.append(self.get_inds(batch).to('cpu').numpy())\n",
    "                if return_true:\n",
    "                    true_list.append(self.get_y(batch).to('cpu').numpy())\n",
    "                pred_list.append(y_pred.to('cpu').numpy() if not isinstance(y_pred,tuple) else\\\n",
    "                                 tuple(y.to('cpu').numpy() for y in y_pred))\n",
    "        pred = np.concatenate(pred_list,0) if not isinstance(pred_list[0],tuple) else\\\n",
    "                tuple(np.concatenate([p[i] for p in pred_list],0) for i in range(len(pred_list[0])))\n",
    "        out=()\n",
    "        if return_inds:\n",
    "            out=out+(np.concatenate(inds_list,0),)\n",
    "        if return_true:\n",
    "            out=out+(np.concatenate(true_list,0),)\n",
    "            \n",
    "        return pred if len(out)==0 else (pred,)+out\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"theNotebook = \" + \"'\"+thename+\"'\";\n",
    "kernel.execute(command);\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted LearnerClass.ipynb to exp/LearnerClass.py\r\n"
     ]
    }
   ],
   "source": [
    "full_notebook_name=theNotebook+'.ipynb'\n",
    "!python notebook2script.py {full_notebook_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
