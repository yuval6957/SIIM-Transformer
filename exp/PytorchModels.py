
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/PytorchModels.ipynb

import torch
import torch.nn as nn
import torch.utils.data as D
import torch.nn.functional as F
import copy
class Noop(nn.Module):
    def __init__(self,*args):
        super(Noop, self).__init__()
    def forward(self,x):
        return x

class NoopAddDim(nn.Module):
    def __init__(self):
        super(NoopAddDim, self).__init__()
    def forward(self,x):
        return x.unsqueeze(-1)

def add_to_dim(x,num_dims,dim=0):
    while len(x.shape)<num_dims:
        x=x.unsqueeze(dim)
    return x

class DummyEmbd(nn.Module):
    def __init__(self,out_size,dtype=torch.float32):
        super(DummyEmbd, self).__init__()
        self.out_size=out_size
        self.dtype=dtype
    def forward(self,x):
        return torch.zeros(x.shape+(self.out_size,),dtype=self.dtype,device=x.device)

def soft_cross_entropy (input, target):
    return  -(target * F.log_softmax (input, dim = 1)).sum(1).mean(0)

class FocalCrossEntropy():
    def __init__(self,gamma):
        self.gamma=gamma

    def __call__(self,pred,target):
        if pred.shape!=target.shape:
            targets = torch.zeros_like(pred)
            targets[torch.arange(target.shape[0]),target]=1
        else:
            targets=target
        return -(torch.pow(1-F.softmax(pred, dim = 1),self.gamma) * targets * F.log_softmax (pred, dim = 1)).sum(1).mean(0)

class ExtraModel(nn.Module):
    def __init__(self,model,last_layer,extras,mid_linear,mlps=[],extra_activation=nn.ReLU(),dropout=0,bn=False,
                 patient_emdb=None,return_features=False):
        super(ExtraModel, self).__init__()
        self.base_model=copy.deepcopy(model)
        self.return_features=return_features
        last = self.base_model._modules[last_layer]
        in_last=last.in_features
        out_last=last.out_features
        bias_last=last.bias
        added_ins=0
        for i,l in enumerate(extras):
            if isinstance(l,(list,tuple)) and len(l)==2:
                self.add_module(f'extra_layers{i}',nn.Embedding(l[0], l[1]))
                added_ins+=l[1]
            elif isinstance(l,int):
                if l>1:
                    self.add_module(f'extra_layers{i}',nn.Sequential(NoopAddDim(),nn.Linear(1, l)))
                    added_ins+=l
                else:
                    self.add_module(f'extra_layers{i}',NoopAddDim())
                    added_ins+=1
            else:
                raise ValueError(f'extras {i} is {l} which is not an interger or a list/tuple of size 2')
        self.extra_linear=nn.Linear(added_ins,mid_linear)
        self.extra_bn=nn.BatchNorm1d(mid_linear) if bn else Noop()
        self.extra_dropout = nn.Dropout(dropout) if dropout>0 else Noop()
        self.patient_embd=patient_emdb if patient_emdb is None else nn.Embedding.from_pretrained(patient_emdb)
        em_size=0 if self.patient_embd is None else patient_emdb.shape[1]
        if len(mlps)==0:
            self.last_linear=nn.Linear(mid_linear+in_last+em_size,out_last,bias=bias_last is not None)
        else:
            nmlps=[mid_linear+in_last+em_size]+mlps
            sq = [x for s in [[nn.Linear(nmlps[i],nmlps[i+1]),
                               extra_activation,
                               nn.BatchNorm1d(nmlps[i+1]) if bn else Noop(),
                               nn.Dropout(dropout) if dropout>0 else Noop()] for i in range(len(mlps))] for x in s]
            self.add_module('mlps',nn.Sequential(*tuple(sq)))
            self.last_linear=nn.Linear(mlps[-1],out_last,bias=bias_last is not None)
        self.extra_activation=extra_activation
        self.base_model._modules[last_layer]=Noop()


    def forward(self,x,*extra):
        x = self.base_model(x)
        if self.patient_embd is not None:
            x=torch.cat([x,self.patient_embd(extra[0])],1)
            extra=extra[1:]
        extra=torch.cat([self.extra_activation(getattr(self,f'extra_layers{i}')(extra[i])) for i in range(len(extra))],1)
        extra=self.extra_activation(self.extra_linear(extra))
        x=torch.cat([x,extra],1)
        if hasattr(self,'mlps'):
            x=self.mlps(x)
        out = self.last_linear(x)
        return (out,x) if self.return_features else out

import math
def calc_positional_encoder(d_model, max_seq_len = 32):
        # create constant 'pe' matrix with values dependant on
        # pos and i
        pe = torch.zeros(max_seq_len, d_model)
        for pos in range(max_seq_len):
            for i in range(0, d_model, 2):
                pe[pos, i] = \
                math.sin(pos / (10000 ** ((2 * i)/d_model)))
                pe[pos, i + 1] = \
                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))
        return pe/(d_model**0.5)


class TransformerModel(nn.Module):
    def __init__(self,in_size,
                 dim_feedforward,
                 n_heads=4,
                 n_encoders=4,
                 num_outputs=8,
                 use_age=True,
                 max_site_num=7,
                 use_sex=True,
                 use_age_diff=True,
                 use_position_enc=False):
        super(TransformerModel, self).__init__()
        self.in_size=in_size
        self.encoder_layer =nn.TransformerEncoderLayer(in_size,
                                                       n_heads,
                                                       dim_feedforward=dim_feedforward)
#        self.decoder_layer =nn.TransformerDecoderLayer(in_size, 4, dim_feedforward=in_size)
        self.encoder=nn.TransformerEncoder(self.encoder_layer, n_encoders)
#        self.decoder=nn.TransformerDecoder(self.decoder_layer, 2)
        self.sex_embd=nn.Embedding(2,in_size) if  use_sex else DummyEmbd(in_size)
        self.age_embd=nn.Sequential(NoopAddDim(),nn.Linear(1,16),nn.ReLU(),nn.Linear(16,in_size)) if use_age else DummyEmbd(in_size)
        self.site_embd=nn.Embedding(max_site_num,in_size) if max_site_num>0 else DummyEmbd(in_size)
        self.age_diff_embd=nn.Sequential(NoopAddDim(),nn.Linear(1,16),nn.ReLU(),nn.Linear(16,in_size)) if use_age else DummyEmbd(in_size)
        self.classifier = nn.Linear(in_size, num_outputs)
        self.pos_embd=calc_positional_encoder(in_size) if use_position_enc else None

    def forward(self, x,sex=None,age=None,site=None,age_diff=None,mask=None):
        if self.pos_embd is not None:
            if self.pos_embd.device!=x.device:
                self.pos_embd = self.pos_embd.to(x.device)
        x = x if sex is None else x + self.sex_embd(sex)
        x = x if age is None else x + self.age_embd(age)
        x = x if site is None else x + self.site_embd(site)
        x = x if age_diff is None else x + self.age_diff_embd(age_diff)
        x = x if self.pos_embd is None else x + self.pos_embd[:x.shape[1]][None]
        x = x if mask is None else x*mask.unsqueeze(-1)
        x = self.encoder(x.permute(1,0,-1)) #,mask=s_mask
        x = x.permute(1,0,-1)
        out = self.classifier(x)
        return out



from collections import OrderedDict
import torch.nn as nn
import torchvision.models as models
import pretrainedmodels
import geffnet
def get_model(model_name,output_size,pretrained=True, extra=None, mid_extra=0,extra_activation=nn.ReLU(),
              mlps=[],dropout=0,bn=False,patient_emdb=None,return_features=False):
    if model_name.startswith('resne'):
        m=getattr(models,model_name)
        model=m(pretrained=pretrained)
        model.fc=nn.Linear(model.fc.in_features,output_size)
        last = 'fc'
    elif model_name.startswith('densenet'):
        m=getattr(models,model_name)
        model=m(pretrained=pretrained)
        model.classifier=nn.Linear(model.classifier.in_features,output_size)
        last='classifier'
    elif model_name.startswith('my_densenet'):
        model=models.DenseNet(32,block_config=(6, 12,32),num_init_features=64,num_classes=output_size)
    elif model_name in ['efficientnet_b0','efficientnet_b1','efficientnet_b2','efficientnet_b3'] or model_name.startswith('tf_'):
        model=geffnet.create_model( model_name, pretrained=pretrained)
        model.classifier=nn.Linear(model.classifier.in_features,output_size)
        last = 'classifier'
    elif model_name=='DCTConvModel':
        model=DCTConvModel()
    elif model_name=='xception':
        model=pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')
        model.last_linear=nn.Linear(model.last_linear.in_features,output_size)
        last='last_linear'

    else:
        raise ValueError('no model named '+model_name)
    if extra is not None and mid_extra>0:
        model= ExtraModel(model,last,extra,mid_linear=mid_extra,extra_activation=extra_activation,
                          mlps=mlps,dropout=dropout,bn=bn,patient_emdb=patient_emdb,return_features=return_features)
    elif len(mlps)>0:
        in_last=model._modules[last].in_features
        out_last=model._modules[last].out_features
        nmlps=[in_last]+mlps
        sq = OrderedDict([x for s in [[(f'mlp_linear{i}',nn.Linear(nmlps[i],nmlps[i+1])),
                           (f'mlp_act{i}',extra_activation),
                           (f'mlp_bn{i}',nn.BatchNorm1d(nmlps[i+1]) if bn else Noop()),
                           (f'mlp_dropout{i}',nn.Dropout(dropout) if dropout>0 else Noop())] for i in range(len(mlps))] for x in s])
        sq['last_linear'] = nn.Linear(nmlps[-1],out_last)
        model._modules[last]=nn.Sequential(sq)

    return model
